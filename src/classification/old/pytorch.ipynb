{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import torchmetrics\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "          TimeStamp      X_gaze      Y_gaze  Pupil radius        DVA\n11917214       2794  520.933333  404.400000        5230.0   5.843608\n11917215       2796  521.466667  405.466667        5229.0   5.820253\n11917216       2798  524.400000  408.200000        5229.0   5.726847\n11917217       2800  529.466667  412.466667        5228.0   5.570816\n11917218       2802  535.200000  416.266667        5227.0   5.405634\n...             ...         ...         ...           ...        ...\n11918089       4544  335.533333  265.200000        4910.0  11.442557\n11918090       4546  335.733333  265.000000        4916.0  11.441182\n11918091       4548  336.800000  265.000000        4921.0  11.419551\n11918092       4550  336.933333  264.933333        4919.0  11.417745\n11918093       4552  336.333333  265.200000        4916.0  11.426326\n\n[880 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimeStamp</th>\n      <th>X_gaze</th>\n      <th>Y_gaze</th>\n      <th>Pupil radius</th>\n      <th>DVA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11917214</th>\n      <td>2794</td>\n      <td>520.933333</td>\n      <td>404.400000</td>\n      <td>5230.0</td>\n      <td>5.843608</td>\n    </tr>\n    <tr>\n      <th>11917215</th>\n      <td>2796</td>\n      <td>521.466667</td>\n      <td>405.466667</td>\n      <td>5229.0</td>\n      <td>5.820253</td>\n    </tr>\n    <tr>\n      <th>11917216</th>\n      <td>2798</td>\n      <td>524.400000</td>\n      <td>408.200000</td>\n      <td>5229.0</td>\n      <td>5.726847</td>\n    </tr>\n    <tr>\n      <th>11917217</th>\n      <td>2800</td>\n      <td>529.466667</td>\n      <td>412.466667</td>\n      <td>5228.0</td>\n      <td>5.570816</td>\n    </tr>\n    <tr>\n      <th>11917218</th>\n      <td>2802</td>\n      <td>535.200000</td>\n      <td>416.266667</td>\n      <td>5227.0</td>\n      <td>5.405634</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11918089</th>\n      <td>4544</td>\n      <td>335.533333</td>\n      <td>265.200000</td>\n      <td>4910.0</td>\n      <td>11.442557</td>\n    </tr>\n    <tr>\n      <th>11918090</th>\n      <td>4546</td>\n      <td>335.733333</td>\n      <td>265.000000</td>\n      <td>4916.0</td>\n      <td>11.441182</td>\n    </tr>\n    <tr>\n      <th>11918091</th>\n      <td>4548</td>\n      <td>336.800000</td>\n      <td>265.000000</td>\n      <td>4921.0</td>\n      <td>11.419551</td>\n    </tr>\n    <tr>\n      <th>11918092</th>\n      <td>4550</td>\n      <td>336.933333</td>\n      <td>264.933333</td>\n      <td>4919.0</td>\n      <td>11.417745</td>\n    </tr>\n    <tr>\n      <th>11918093</th>\n      <td>4552</td>\n      <td>336.333333</td>\n      <td>265.200000</td>\n      <td>4916.0</td>\n      <td>11.426326</td>\n    </tr>\n  </tbody>\n</table>\n<p>880 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "with open('sequences.pkl', 'rb') as file:\n",
    "    # Load the list of tuples from the file\n",
    "    sequences = pickle.load(file)\n",
    "sequences[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(3964, 991)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq, test_seq = train_test_split(sequences, random_state=420, test_size=0.2)\n",
    "len(train_seq), len(test_seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class EyeTrackingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.sequences[idx]\n",
    "        return dict(\n",
    "            sequences=torch.Tensor(sequence.to_numpy()),\n",
    "            label=torch.tensor(label).long()\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class EyeTrackingDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_sequence, test_sequence, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_sequence = train_sequence\n",
    "        self.test_sequence = test_sequence\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_sequence = EyeTrackingDataset(self.train_sequence)\n",
    "        self.test_sequence = EyeTrackingDataset(self.test_sequence)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_sequence, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_sequence, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_sequence, batch_size=self.batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "N_EPOCHS = 250\n",
    "BATCH_SIZE = 64 # ?\n",
    "\n",
    "data_module = EyeTrackingDataModule(train_seq, test_seq, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_classes, n_hidden=256, n_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.75\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "\n",
    "        out = hidden[-1]\n",
    "        return self.classifier(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class EyeTrackingPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features:int, n_classes:int):\n",
    "        super().__init__()\n",
    "        self.model = SequenceModel(n_features, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequences\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_accuracy = torchmetrics.functional.accuracy(predictions, labels, task=\"binary\")\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"train_accuracy\", step_accuracy, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"accuracy\": step_accuracy}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequences\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_accuracy = torchmetrics.functional.accuracy(predictions, labels, task=\"binary\")\n",
    "\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"validation_accuracy\", step_accuracy, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"accuracy\": step_accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequences\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_accuracy = torchmetrics.functional.accuracy(predictions, labels, task=\"binary\")\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"test_accuracy\", step_accuracy, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"accuracy\": step_accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr = 0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = EyeTrackingPredictor(\n",
    "    n_features=len(sequences[0][0].columns),\n",
    "    n_classes=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-d0cc9c728f22ba2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-d0cc9c728f22ba2\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"validation_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"EyeTracking\")\n",
    "\n",
    "trainer = pl.Trainer(logger=logger, callbacks=checkpoint_callback, max_epochs=N_EPOCHS, enable_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\Users\\Daniel\\projects\\EyeTrackingForEpisodicMemory\\Gaze\\src\\classification\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | SequenceModel    | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.290     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71f7ca96d9234c5898e20bc048b65505"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cc630225f334463bef8578902eb64f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "374a87bf3a7546e3bfc8ca5cd55dbfc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 62: 'validation_loss' reached 0.69299 (best 0.69299), saving model to 'C:\\\\Users\\\\Daniel\\\\projects\\\\EyeTrackingForEpisodicMemory\\\\Gaze\\\\src\\\\classification\\\\checkpoints\\\\best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "410f879640f94d1a8df61fbbeee26d41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 124: 'validation_loss' reached 0.69248 (best 0.69248), saving model to 'C:\\\\Users\\\\Daniel\\\\projects\\\\EyeTrackingForEpisodicMemory\\\\Gaze\\\\src\\\\classification\\\\checkpoints\\\\best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd2776c561f4491b8bea1f2ad6ba08a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 186: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a049b4a02444ee2a86fd9a9edb0f6aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 248: 'validation_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
